{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252197,"status":"ok","timestamp":1706225248287,"user":{"displayName":"Federica Manzi","userId":"09301508044361557485"},"user_tz":-60},"id":"R8w2XDBJKk31","outputId":"7b68d213-8303-4b2d-c792-d4738ff4359a"},"outputs":[],"source":["!pip install datasets\n","!pip install crosslingual-coreference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18234,"status":"ok","timestamp":1706225266503,"user":{"displayName":"Federica Manzi","userId":"09301508044361557485"},"user_tz":-60},"id":"82XzfMbdPTDj","outputId":"02a95dd7-ab76-4470-f31c-ff6902ba3ba2"},"outputs":[],"source":["from crosslingual_coreference import Predictor\n","from collections import defaultdict\n","import ast\n","import re\n","import torch\n","import nltk\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706225266503,"user":{"displayName":"Federica Manzi","userId":"09301508044361557485"},"user_tz":-60},"id":"baY_hRwQPYGl","outputId":"f8cb0d48-65de-4f12-ce78-194ce7471e80"},"outputs":[],"source":["if torch.cuda.is_available():\n","    # Set the device to GPU\n","    device = torch.device(\"cuda\")\n","    print(\"Using GPU.\")\n","else:\n","    # Set the device to CPU\n","    device = torch.device(\"cpu\")\n","    print(\"No GPU available, using CPU.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5179992,"status":"ok","timestamp":1706233489119,"user":{"displayName":"Federica Manzi","userId":"09301508044361557485"},"user_tz":-60},"id":"lbCsf_DpPear"},"outputs":[],"source":["def extract_crossreferences(text):\n","  \"\"\"\n","    Extracts cross-references from a given text.\n","\n","    @:param text: The text from which to extract cross-references.  \n","\n","    @:return clusters: The extracted cross-references.\n","\n","  \"\"\"\n","  # In this colab we are using 1 GPU (torch.cuda.device_count()), so to use the GPU we need to specify device = 0\n","  # device = -1 corresponds on running on CPU\n","  predictor = Predictor(\n","      language=\"it_core_news_lg\", device=0, model_name=\"minilm\"\n","  )\n","\n","  clusters = predictor.predict(text)\n","  del clusters[\"resolved_text\"] \n","\n","  coreferences = adjust_coreferences(clusters)\n","\n","  return coreferences\n","\n","\n","def adjust_coreferences(coreferences):\n","  \"\"\"\n","    Adjusts the given coreferences (avoid long nominal phrases and just extract names)\n","\n","    @:param coreferences: The coreferences to be adjusted.    \n","\n","    @:return new_coreferences: The adjusted coreferences.\n","\n","  \"\"\"\n","\n","  new_coreferences = {\"cluster_heads\": defaultdict(), \"clusters\":[], \"span2head\":defaultdict()}\n","  for coreference, span in coreferences[\"cluster_heads\"].items():\n","    \n","    is_up = any(char.isupper() for char in coreference)\n","    # Exclude coreferences that do not contain any uppercase letters (probably no names)\n","    if not is_up:\n","      continue\n","    # Try to reduce nominal phrases and only extract the names\n","    elif len(coreference) > 35:\n","      pattern = r'[A-Z](?:[a-z]+|[A-Z]+)(?:\\s[A-Z][a-z]+)?\\b'\n","\n","      for matchh in re.finditer(pattern, coreference):\n","        start_match = matchh.start()\n","        end_match = matchh.end()\n","        new_coref = matchh.group()\n","        new_start_span = span[0]+start_match\n","        new_end_span = new_start_span + (end_match - start_match)\n","        new_span = [new_start_span, new_end_span]\n","        # Adjust coreferences spans\n","        new_coreferences[\"cluster_heads\"][new_coref] = new_span\n","        new_coreferences[\"span2head\"][(new_span[0], new_span[1])] = new_coref\n","\n","        for cluster in coreferences[\"clusters\"]:\n","          if cluster[0][0] == span[0] and cluster[0][1] == span[1]:\n","            new_cluster = [new_span] + cluster[1:]\n","            new_coreferences[\"clusters\"].append(new_cluster)\n","    else:\n","      new_coreferences[\"cluster_heads\"][coreference] = span\n","      new_coreferences[\"span2head\"][(span[0], span[1])] = coreference\n","      for cluster in coreferences[\"clusters\"]:\n","        if cluster[0][0] == span[0] and cluster[0][1] == span[1]:\n","          new_coreferences[\"clusters\"].append(cluster)\n","\n","  return new_coreferences\n","\n","def update_data(text, labels, coreferences):\n","\n","  \"\"\"\n","  Updates the given text by:\n","   1) sentence tokenizing the text\n","   2) adding the coreferences at the beginning of the sentence , if needed\n","   3) adjust the labels spans to refer to the sentence in which they occurr and not the whole text and taking into consideration the added coreferences\n","\n","  @:param\n","    text: The text where coreferences need to be added\n","    labels: The entities in the text\n","    coreferences: The coreferences extracted from the text.\n","\n","  @return: new_sentences, new_labels, new_entities\n","\n","  \"\"\"\n","\n","  # Sentence tokenize the text\n","  tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n","  # Get the spans for each sentence\n","  spans = list(tokenizer.span_tokenize(text))\n","  # Extract the actual sentences\n","  sentences = [text[start:end] for start, end in spans]\n","\n","  # Initialize the new labels per sentence that we will use for the binary classification\n","  new_labels =[0]*len(sentences)\n","  new_sentences = []\n","  new_entities = []\n","\n","  for i in range(len(sentences)):\n","    # Check if there are coreferences that refer to this specific text\n","    sentence = sentences[i]\n","    has_coref = False\n","    new_sent = sentence\n","\n","    if coreferences:\n","\n","      for cluster in coreferences[\"clusters\"]:\n","        for coreference in cluster[1:]:\n","\n","      # If there are coreferences, add the head of the reference at the beginning of the sentence in square brackets \n","          if spans[i][0] <= coreference[0] <= coreference[1] <= spans[i][1]:\n","            # Add the reference at the beginning of the sentence\n","            if coreferences[\"span2head\"][(cluster[0][0], cluster[0][1])] not in new_sent:\n","              new_sent = \"[\" + coreferences[\"span2head\"][(cluster[0][0], cluster[0][1])] + \"] \" + new_sent\n","              has_coref = True\n","\n","    # Calculate length of added coreferences\n","    coref_length = len(new_sent) - len(sentences[i])\n","\n","    if not has_coref:\n","      new_sentences.append(sentence)\n","    else:\n","      new_sentences.append(new_sent)\n","\n","    new_entity_group = []\n","\n","    # Check if there are entities inside that sentence --> if there are, assign a label 1, 0 otherwise\n","    for label in labels:\n","\n","      if spans[i][0] <= label[\"start_offset\"] <= label[\"end_offset\"] <= spans[i][1] :\n","        # Change spans of entities to refer to the sentence length and add the length of the coreference\n","        new_start_offset = label[\"start_offset\"] - spans[i][0] + coref_length\n","        new_end_offset = label[\"end_offset\"] - spans[i][0] + coref_length\n","        new_entity = {\"label\":label[\"label\"], \"start_offset\":new_start_offset, \"end_offset\": new_end_offset}\n","        new_labels[i] = 1\n","\n","        new_entity_group.append(new_entity)\n"," \n","\n","    new_entities.append(new_entity_group)\n","\n","  return new_sentences, new_labels, new_entities\n","\n","def prepare_data(data):\n","\n","  \"\"\"\n","  Prepare data for further processing.\n","\n","  @:param data: The data of the current fold that need to be modified.\n","\n","  @return: new_data: preprocessed dataset, sentence tokenized and with coreferences added\n","\n","  \"\"\"\n","\n","  coreferences = []\n","  coreferences = extract_crossreferences(data[\"words\"])\n","  sents, labels, entities = update_data(data[\"words\"], data[\"labels\"], coreferences)\n","  new_data = []\n","  for sent, label, entity in zip(sents, labels, entities):\n","    new_data.append({\"text\": sent, \"label\": label, \"entities\":entity})\n","\n","  return new_data\n","\n","def main():\n","  # Load and apply coreference to documents in each fold defined for the baseline\n","  for i in range(5):\n","    fold = []\n","\n","    with open(\"dataset/baseline/fold0\"+str(i)+\".json\", \"r\", encoding=\"utf-8\") as fold_file:\n","      fold_file = json.load(fold_file)\n","\n","    fold.extend(fold_file[\"Data\"])\n","\n","    new_data = []\n","    # Divide the text into sentences and apply coreference\n","    for data in fold:\n","      new_data.extend(prepare_data(data))\n","    dataset = {\"Data\": new_data}\n","\n","    # Store the new fold in json files\n","    with open(\"dataset/pipeline/data_sent_coref_fold\"+str(i)+\".json\", \"w\", encoding=\"utf-8\") as out_file:\n","      json_data = json.dumps(dataset, ensure_ascii=False)\n","      out_file.write(json_data)\n","\n","main()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO1ckyku0/zMaDcST1bvzI3","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
